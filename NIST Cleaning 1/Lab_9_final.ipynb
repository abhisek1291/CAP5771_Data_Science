{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9b4daacd-1a19-4b07-8534-fb1dbdfff53d"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    "div.cell { /* Tunes the space between cells */\n",
    "margin-top:1em;\n",
    "margin-bottom:1em;\n",
    "}\n",
    "\n",
    "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
    "font-size: 2.2em;\n",
    "line-height:1.4em;\n",
    "text-align:center;\n",
    "}\n",
    "\n",
    "div.text_cell_render h2 { /*  Parts names nearer from text */\n",
    "margin-bottom: -0.4em;\n",
    "}\n",
    "\n",
    "\n",
    "div.text_cell_render { /* Customize text cells */\n",
    "font-family: 'Georgia';\n",
    "font-size:1.2em;\n",
    "line-height:1.4em;\n",
    "padding-left:3em;\n",
    "padding-right:3em;\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install jenks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3d50cb35-e6fc-4b7b-a097-29b61d2f3379"
    }
   },
   "source": [
    "# Loading the data into a dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "feb5e1f7-d134-4bc9-b271-6e174bfebcf2"
    }
   },
   "source": [
    "<b> Reading data from the required CSV files. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d673c064-db6d-455f-8530-996372966400"
    }
   },
   "outputs": [],
   "source": [
    "flow = pd.read_csv(\"flow.tsv\", na_values=['-'], delimiter=\"\\t\", error_bad_lines=False)\n",
    "occupancy = pd.read_csv(\"occupancy.tsv\", na_values=['-'], delimiter=\"\\t\", error_bad_lines=False)\n",
    "speed = pd.read_csv(\"speed.tsv\", na_values=['-'], delimiter=\"\\t\", error_bad_lines=False)\n",
    "timestamp = pd.read_csv(\"timestamp.tsv\", na_values=['-'], delimiter=\"\\t\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5ec5ba5e-d329-44ae-9ee7-f00bc4d1d32e"
    }
   },
   "source": [
    "<b> Concating the values of the columns into a single dataframe </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "0a63890c-9317-4cb0-bffa-0747b47c33b5"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([flow,occupancy,speed,timestamp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2804fb98-16f0-4c98-857d-75a32e359662"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flow1</th>\n",
       "      <th>flow2</th>\n",
       "      <th>flow3</th>\n",
       "      <th>occupancy1</th>\n",
       "      <th>occupancy2</th>\n",
       "      <th>occupancy3</th>\n",
       "      <th>speed1</th>\n",
       "      <th>speed2</th>\n",
       "      <th>speed3</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>255</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2006-09-01T00:01:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>255</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2006-09-01T00:02:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-8</td>\n",
       "      <td>3</td>\n",
       "      <td>255</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2006-09-01T00:03:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>255</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2006-09-01T00:04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>255</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2006-09-01T00:05:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flow1  flow2  flow3  occupancy1  occupancy2  occupancy3  speed1  speed2  \\\n",
       "0      9      4    255         4.0         2.0         4.0    67.0    60.0   \n",
       "1     11     10    255         4.0         5.0         4.0    66.0    64.0   \n",
       "2     -8      3    255         3.0         1.0         4.0    71.0    63.0   \n",
       "3      5      9    255         2.0         3.0         4.0    65.0    70.0   \n",
       "4      7     10    255         2.0         4.0         4.0    66.0    64.0   \n",
       "\n",
       "   speed3            timestamp  \n",
       "0     0.0  2006-09-01T00:01:07  \n",
       "1     0.0  2006-09-01T00:02:07  \n",
       "2     0.0  2006-09-01T00:03:07  \n",
       "3     0.0  2006-09-01T00:04:07  \n",
       "4     0.0  2006-09-01T00:05:08  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['flow1','flow2', 'flow3','occupancy1','occupancy2', 'occupancy3','speed1','speed2', 'speed3','timestamp']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f0a0f397-ba69-4cfa-9712-f834d6afefe4"
    }
   },
   "source": [
    "<b> Implementing vector seperation. Dividing the single dataframe into seperate dataframes. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "cacc7ed6-f993-465e-b758-f1951beb8025"
    }
   },
   "outputs": [],
   "source": [
    "df1 =  pd.DataFrame({\"flow\" : df.flow1, \"occupancy\" : df.occupancy1,\"speed\" : df.speed1,\"timestamp\" : df.timestamp,\"detector\" : \"x\"})\n",
    "df2 = pd.DataFrame({\"flow\" : df.flow2, \"occupancy\" : df.occupancy2,\"speed\" : df.speed2,\"timestamp\" : df.timestamp,\"detector\" : \"y\"})\n",
    "df3 = pd.DataFrame({\"flow\" : df.flow3, \"occupancy\" : df.occupancy3,\"speed\" : df.speed3,\"timestamp\" : df.timestamp,\"detector\" : \"z\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1e12c4ae-da73-404d-ae1c-1892a99ccb4b"
    }
   },
   "source": [
    "<b> Concatenating the dataframes </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2abfdeaf-00da-4a4f-8786-f6663e34f476"
    }
   },
   "outputs": [],
   "source": [
    "frames = [df1, df2, df3]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Sorting on timestamp </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "36bd5b56-4114-49bd-bead-aeddb429498d"
    }
   },
   "outputs": [],
   "source": [
    "result = result.sort_values(\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <b>Finding outlier values and cleaning the occupancy column </b>\n",
    " \n",
    " Observations :-\n",
    "        As we can see from the graph below, the occupancy values in the dataset increase consistently over a range\n",
    "       of 0 to 100 but change drastically after that. This indicates possible outliers and wrong data.From the above\n",
    "       observation, we conclude that even during a traffic jam the occupancy values should not be\n",
    "       greater than a threshold of 120. So, we keep a variable jam_occupancy_threshold which holds a value of 120.\n",
    "       This value helps us in identifying values which can be ignored while computing probablity distribution. We \n",
    "       can safely assume a probability density of 0 for rows carying such values. \n",
    "       \n",
    "# Occupancy Outlier Graph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "80522021-2c18-4083-9b39-7989e067747a"
    }
   },
   "outputs": [],
   "source": [
    "val = np.sort(result[\"occupancy\"].unique())\n",
    "plt.plot(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Finding outlier values and cleaning the Speed column </b>\n",
    " \n",
    "Observations :- The graph below shows that the data is consistent till value 120 and gets skwed above that i.e speed > 120 mph. This makes perfect sense as speeds above are not practically feasible considering the speed limit laws.\n",
    "\n",
    "# Speed Outlier Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8ba3b08d-4b38-4b39-b944-06e01ed5691a"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim(550, 650)\n",
    "plt.plot(result[\"speed\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Wrong Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> There are several rows which can be easily identified as wrong values. We will be identifying these values below and will work on removing this data </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Observation :- </b> \n",
    "\n",
    "In the below cell we try to remove all rows where speed is less than 0 or the flow is less than zero. This is practically infeasible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e622fb1a-9f5e-4933-866d-f76a767f928d"
    }
   },
   "outputs": [],
   "source": [
    "removeResult1 = result[(result['speed'] < 0) | (result['flow'] < 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we try to remove all rows where flow is equal to zero and speed is greater than zero. This is also practically not possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removeResult2 = result[(result['flow'] == 0) & (result['speed'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we work on defining the Jam occupancy threshold which we had earlier defined while checking occupancy outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jam_occupancy_threshold = 120\n",
    "removeResult3 = result[((result['speed'] == 0) & \n",
    "                            (result['flow'] == 0) & \n",
    "                            (result['occupancy'] != 0) & \n",
    "                            (result['occupancy'] < jam_occupancy_threshold))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we work on removing rows which have a speed of zero and an occupancy of zero but the flow is greater than zero. This is not possible practically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removeResult5 = result[(result['speed'] == 0) & (result['flow'] != 0) & (result[\"occupancy\"] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insignificant Data\n",
    "\n",
    "In the below cell we work on defining the rows which have insignificant contribution to the data. This includes the condition where speed is zero, flow is zero and occupancy is also zero. This is a valid senario where there are no vehicles on the road. By running few observations we found more than 45,00,000 rows with such data in the first data set. Despite being correct, this data skews the probability distribution for all the other rows. Also, since we are interested in understanding the true outliers we can ignore these rows. By this step, we get a remarkable increase in the efficiency with which we can determine the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removeResult4 = result[(result['speed'] == 0) & (result['flow'] == 0) & (result[\"occupancy\"] == 0)]\n",
    "removeResult6 = result[(pd.isnull(result['speed'])) | (pd.isnull(result['occupancy'])) | (pd.isnull(result['flow']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally removing all the wrong and insignificant data from the dataframe which would not be used for processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = pd.concat([result,removeResult1,removeResult1]).drop_duplicates(keep=False)\n",
    "result = pd.concat([result,removeResult2,removeResult2]).drop_duplicates(keep=False)\n",
    "result = pd.concat([result,removeResult3,removeResult3]).drop_duplicates(keep=False)\n",
    "# result = pd.concat([result,removeResult4,removeResult4]).drop_duplicates(keep=False)\n",
    "result = pd.concat([result,removeResult5,removeResult5]).drop_duplicates(keep=False)\n",
    "# result = pd.concat([result,removeResult6,removeResult6]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are indexing the data based on the timestamp and detector column value. This helps us in understanding how the data can be processed further for probability density calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e1577966-8610-4beb-b467-d6163d9a0abf"
    }
   },
   "outputs": [],
   "source": [
    "res = result.set_index([\"timestamp\",\"detector\"]).sort_index()\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Probability Density Calculation # Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> sampleData Function divides data into classes </h3>\n",
    "\n",
    "In this function we initialize variable <i><b>\"flowbins\"</b></i> which would contain the bins obtained after dividing the dataset based on the specified <i><b>\"threshold\"</b></i>. We also use the <i><b>\"columnName\"</b></i> variable to specify the column for which we are putting the values into the bins. The data falling between the specified <i><b>threshold</b></i> would be put into the same bin.\n",
    "\n",
    "<i> For e.g :- </i>\n",
    "\n",
    "If we specify the <i><b>threshold</b></i> for <i><b>flow</b></i> column as <i><b>15</b></i>, then the flow column would be divided into <i><b>20</b></i> different bins in the manner 0-14, 14-29 ....\n",
    "Similarly for all other columns we would have different values in <i><b>20</b></i> different bins based on <i><b>threshold</b></i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from jenks import jenks\n",
    "import numpy as np\n",
    "def goodness_of_variance_fit(array, classes):\n",
    "    # get the break points\n",
    "    classes = jenks(array, classes)\n",
    "\n",
    "    # do the actual classification\n",
    "    classified = np.array([classify(i, classes) for i in array])\n",
    "\n",
    "    # max value of zones\n",
    "    maxz = max(classified)\n",
    "\n",
    "    # nested list of zone indices\n",
    "    zone_indices = [[idx for idx, val in enumerate(classified) if zone + 1 == val] for zone in range(maxz)]\n",
    "\n",
    "    # sum of squared deviations from array mean\n",
    "    sdam = np.sum((array - array.mean()) ** 2)\n",
    "\n",
    "    # sorted polygon stats\n",
    "    array_sort = [np.array([array[index] for index in zone]) for zone in zone_indices]\n",
    "\n",
    "    # sum of squared deviations of class means\n",
    "    sdcm = sum([np.sum((classified - classified.mean()) ** 2) for classified in array_sort])\n",
    "\n",
    "    # goodness of variance fit\n",
    "    gvf = (sdam - sdcm) / sdam\n",
    "\n",
    "    return gvf\n",
    "\n",
    "def classify(value, breaks):\n",
    "    for i in range(1, len(breaks)):\n",
    "        if value < breaks[i]:\n",
    "            return i\n",
    "    return len(breaks) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result[result['speed'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def sampleKMeans(data, columnName):\n",
    "#     random_state = np.random.RandomState(0)\n",
    "#     data = data.sort_values(columnName)\n",
    "    column_names = ['flow', 'occupancy', 'speed']\n",
    "    flowData = data[column_names]\n",
    "    km = KMeans(n_clusters=8000, verbose=1).fit(flowData)\n",
    "    return km\n",
    "\n",
    "km = sampleKMeans(result, 'speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "def sampleCluster(data, columnName):\n",
    "    data = data.sort_values(columnName)\n",
    "    flowData = data[columnName].reshape(-1,1)\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=3).fit(flowData)\n",
    "    s = np.linspace(0,300)\n",
    "    e = kde.score_samples(s.reshape(-1,1))\n",
    "    plot(s, e)\n",
    "    return s,e\n",
    "\n",
    "s,e = sampleCluster(result, 'speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print result.loc((0))\n",
    "test = result\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test.index.min()\n",
    "# df1.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print km.labels_\n",
    "# {i: np.where(km.labels_ == i)[0] for i in range(km.n_clusters)}\n",
    "\n",
    "# for i in range(km.n_clusters):\n",
    "#     print np.where(km.labels_ == i)[0]\n",
    "\n",
    "# for i in range(km.n_clusters):\n",
    "#     print np.where(km.labels_ == i)[0].max()\n",
    "for i in range(20):\n",
    "    ids = np.where(km.labels_ == i)[0]\n",
    "    df1 = pd.DataFrame(ids)\n",
    "    df1.columns = ['id']\n",
    "    df1.set_index('id')\n",
    "    # df1.index\n",
    "    joined = test.merge(df1, left_index=True, right_index=True, how = 'inner')\n",
    "    print joined['speed'].mean()\n",
    "# result.head()\n",
    "# joined = df1.merge(result, how='inner')\n",
    "# joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "fc67e0a3-4941-4a03-b6f1-dfd1040728c8"
    }
   },
   "outputs": [],
   "source": [
    "def sampleData(data, threshold, columnName):\n",
    "    flowBins = np.zeros(shape=(20,), dtype = object)\n",
    "    data = data.sort_values(columnName)\n",
    "    difference = threshold\n",
    "    high  = difference\n",
    "    low = 0;\n",
    "    for i in range(0, 20, 1):\n",
    "        b = data[data[columnName] < high]\n",
    "        b = b[b[columnName] >= low]\n",
    "        flowBins[i] = b\n",
    "        low = high\n",
    "        high = high + difference\n",
    "    return flowBins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling sampleData function for  <i><b>flow</b></i> column with a <i><b>threshold</b></i> value of <i><b>15</b></i>. This signifies that with a flow in range of <i><b>0 - 300</b></i> we would divide the flow column into <i><b>20 bins</b></i> containing datasets beween the specified range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "bea8e9ee-a9fa-4bc2-bd6e-d7c54e0d9ce6"
    }
   },
   "outputs": [],
   "source": [
    "flowBins = np.zeros(shape=(20,), dtype = object)\n",
    "flow_threshold = 15;\n",
    "flowBins = sampleData(result, flow_threshold ,\"flow\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling sampleData function for  <i><b>occupancy</b></i> column with a <i><b>threshold</b></i> value of <i><b>6</b></i>. This signifies that with a subset of the orginal dataset, containing flow in range of <i><b>0 - 15</b></i> in first bin, we would further divide the first flow bin into <i><b>20</b></i> bins with a threshold of <i><b>6</b></i> for occupancy i.e  <i><b>flow_occupancy_bin[1]</b></i> will contain 20 bins with each bin conatining values between 0-6, 7-14 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "53edbfe6-45c5-4e31-8a10-4f4c9ad5c3bd"
    }
   },
   "outputs": [],
   "source": [
    "flow_occupancy_bin = np.ndarray(shape=(20,20), dtype = object)\n",
    "occupancy_threshold = 6;\n",
    "for i in range(0,20,1):\n",
    "    data = pd.DataFrame(flowBins[i])\n",
    "    flow_occupancy_bin[i] = sampleData(data, 6 ,\"occupancy\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h5> Finally .... </h5>\n",
    "<br>\n",
    "Calling sampleData function for  <i><b>speed</b></i> column with a <i><b>threshold</b></i> value of <i><b>13</b></i>. This signifies that with a subset of the orginal dataset, containing flow in range of <i><b>0 - 15</b></i> in first bin, and occupancy in the first bin in the range of <i><b>0 - 6</b></i>, we would further divide the first flow_occupancy_bin into <i><b>20</b></i> bins with a threshold of <i><b>6</b></i> for speed i.e  <i><b>flow_occupancy_speed_bin[1]</b></i> will contain 20 bins for speed with each bin conatining values between 0-13, 14-28 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5751f54e-6fee-41a7-8c69-0bf12414ef75"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "flow_occupancy_speed_bin = np.ndarray(shape=(20,20,20), dtype = object)\n",
    "speed_threshold = 13;\n",
    "\n",
    "for i in range(0,20,1):\n",
    "    for j in range(0,20,1):\n",
    "        data = flow_occupancy_bin[i][j];\n",
    "        flow_occupancy_speed_bin[i][j] = sampleData(data, 13 ,\"speed\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Iterating the flow_occupancy_speed_bin to compute probability density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Possibly the most important step.... </h4>\n",
    "\n",
    "<br> In this step we find the probability density of the bins using the formula \n",
    "\n",
    "   <br><center><b><i> probability = points_inside_box / float(total_points * volume) </i></b></center>\n",
    "   <br> We iterate over the total number of available bins i.e 20 bins for flow, 20 bins for occupancy and 20 bins for speed to calculate total points_inside_box. Further we have already calculated the volume of each box i.e <b><i> volume = flow_threshold </i>x <i>occupancy_threshold</i> x <i>speed_threshold </i></b>. The total rows in the dataset is being calculated using <b><i> total_points = len(result) </i></b> which we plug into the above formula. This calculates the probability of each bin.\n",
    "   \n",
    "   <br> NOW...WE FACED PERFORMANCE ISSUES.....  :( :( \n",
    "   <br> WHAT TO DO NOW....\n",
    "   <br><br> The issue occoured while we were trying to merge back the probability column into a new dataframe to create our final result. It took almost half-hour to run through a single bin and merge it back to our existing dataframe. This way it would have taken days to complete this dataset. We searched and found this to be a drawback of dataframes.\n",
    "   http://stackoverflow.com/questions/31860671/pandas-append-perfomance-concat-append-using-larger-dataframes\n",
    "   \n",
    "   <br> THEN CAME OUR SAVIOUR....TERMINAL :) :) \n",
    "   <br><br> We started saving all the bins to the file system i.e in CSV files. Using the capability of IPython notebook to run commands on command line directly we created a new directory and saved all our output to that. Next we merged all the files into one file <b><i> merged.csv using terminal cat command</i></b>. Then we delete the output folder containing all the intermediate files. \n",
    "   <br> <b> This way we completed our file operations in 2-3 mins. :) </b>\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ee87d7e8-5676-426a-92ee-c250a8562613"
    }
   },
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame(columns = [\"detector\",\"flow\",\"occupancy\",\"speed\",\"timestamp\", \"probability\"])\n",
    "total_points = len(result)\n",
    "volume = flow_threshold * occupancy_threshold * speed_threshold\n",
    "for i in range(0,20,1):\n",
    "    for j in range(0,20,1):\n",
    "        for k in range(0,20,1):\n",
    "            prob_data = flow_occupancy_speed_bin[i][j][k]\n",
    "            points_inside_box = prob_data.shape[0]\n",
    "            probability = points_inside_box/float(total_points * volume)\n",
    "            prob_data[\"probability\"] = probability\n",
    "            prob_data.to_csv(\"output/out\" + str(i) + '-' + str(j) + '-' + str(k) + \".csv\",sep=\"\\t\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "992aafc5-532e-4b23-929a-bfd06f654eea"
    }
   },
   "outputs": [],
   "source": [
    "! cat output/*.csv > merged.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "09eb403f-89fc-4298-bf09-cd08f592ea17"
    }
   },
   "outputs": [],
   "source": [
    "merged = pd.read_csv(\"merged.csv\", na_values=['-'], delimiter=\"\\t\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8a3ac2dc-3579-4ba0-9276-e9c2a2cf7685"
    }
   },
   "outputs": [],
   "source": [
    "removeHeader = merged[merged[\"probability\"] == \"probability\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7b719e31-359f-4ff5-994e-f87f06fb2f38"
    }
   },
   "outputs": [],
   "source": [
    "merged = pd.concat([merged,removeHeader,removeHeader]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "11585525-c3b6-4374-abe1-1058180d98e6"
    }
   },
   "outputs": [],
   "source": [
    "merged.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "45a2f322-aed3-4427-b189-685fdc4ad63c"
    }
   },
   "outputs": [],
   "source": [
    "merged[\"probability\"] = merged[\"probability\"].apply(pd.to_numeric)\n",
    "merged = merged.sort_values([\"probability\"])\n",
    "\n",
    "final_columns = ['flow', 'speed', 'occupancy', 'probability']\n",
    "merged = merged[final_columns]\n",
    "final_result = pd.DataFrame(columns = [\"flow\",\"speed\",\"occupancy\", \"probability\"])\n",
    "\n",
    "for i in range(0,len(merged),100):\n",
    "    final_result = final_result.append(merged.iloc[[i]])\n",
    "final_result.to_csv(\"1160.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "15218d3d-9f24-4d3b-8aee-ca1bda73eaae"
    }
   },
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f6416dca-6f14-4676-9c9b-d56fae549a4c"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
